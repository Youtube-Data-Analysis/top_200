{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read in the list of videos contained in the csv\n",
    "df = pd.read_csv('vidz.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop columns with redundant information\n",
    "df = df.drop(columns=['comments_disabled', 'ratings_disabled' ,'thumbnail_link'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.categoryId = df.categoryId.astype('object')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 200 entries, 0 to 199\n",
      "Data columns (total 12 columns):\n",
      " #   Column         Non-Null Count  Dtype  \n",
      "---  ------         --------------  -----  \n",
      " 0   video_id       200 non-null    object \n",
      " 1   title          200 non-null    object \n",
      " 2   publishedAt    200 non-null    object \n",
      " 3   channelTitle   200 non-null    object \n",
      " 4   categoryId     200 non-null    int64  \n",
      " 5   trending_date  200 non-null    object \n",
      " 6   tags           200 non-null    object \n",
      " 7   view_count     200 non-null    int64  \n",
      " 8   likes          200 non-null    int64  \n",
      " 9   comment_count  199 non-null    float64\n",
      " 10  description    200 non-null    object \n",
      " 11  duration       200 non-null    object \n",
      "dtypes: float64(1), int64(3), object(8)\n",
      "memory usage: 18.9+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert to datetime --> ppublised at, trending date, duration (timedelta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     2022-10-26 22:00:09+00:00\n",
       "1     2022-10-26 15:58:27+00:00\n",
       "2     2022-10-27 00:18:46+00:00\n",
       "3     2022-10-27 00:00:09+00:00\n",
       "4     2022-10-26 05:06:26+00:00\n",
       "                 ...           \n",
       "195   2022-10-21 20:48:29+00:00\n",
       "196   2022-10-20 20:00:01+00:00\n",
       "197   2022-10-21 04:01:22+00:00\n",
       "198   2022-10-23 16:00:08+00:00\n",
       "199   2022-10-20 09:43:11+00:00\n",
       "Name: publishedAt, Length: 200, dtype: datetime64[ns, UTC]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#convert publishedAT to datetime column\n",
    "df.publishedAt = pd.to_datetime(df.publishedAt, utc=True)\n",
    "df.publishedAt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set date to next day in order to capture videos released at different times in different time zones\n",
    "df.trending_date = '22.28.10'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     2022-10-28 00:00:00+00:00\n",
       "1     2022-10-28 00:00:00+00:00\n",
       "2     2022-10-28 00:00:00+00:00\n",
       "3     2022-10-28 00:00:00+00:00\n",
       "4     2022-10-28 00:00:00+00:00\n",
       "                 ...           \n",
       "195   2022-10-28 00:00:00+00:00\n",
       "196   2022-10-28 00:00:00+00:00\n",
       "197   2022-10-28 00:00:00+00:00\n",
       "198   2022-10-28 00:00:00+00:00\n",
       "199   2022-10-28 00:00:00+00:00\n",
       "Name: trending_date, Length: 200, dtype: datetime64[ns, UTC]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#convert trending to datetime column\n",
    "df.trending_date = pd.to_datetime(df.trending_date, format='%y.%d.%m', utc=True)\n",
    "df.trending_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import tzinfo inorder to strip time zone information from published at. \n",
    "#this makes it a \"naive\" datetime object. may want to change this approach\n",
    "# from datetime import tzinfo\n",
    "\n",
    "# df.loc[1,'publishedAt'].replace(tzinfo=None)\n",
    "#strips the timezone from each row\n",
    "# for n in range(0,200):\n",
    "#     df.loc[n,'publishedAt']= df.loc[n,'publishedAt'].replace(tzinfo=None)\n",
    "# df.loc[10, ['publishedAt']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create age column. may want to give timezone info to trending date instead of removing it from pblishedAt\n",
    "df['age']=(df.trending_date - df.publishedAt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10   0 days 19:55:54\n",
       "18   0 days 19:59:53\n",
       "15   0 days 21:59:50\n",
       "11   0 days 23:40:21\n",
       "2    0 days 23:41:14\n",
       "3    0 days 23:59:51\n",
       "21   0 days 23:59:52\n",
       "6    1 days 01:48:57\n",
       "0    1 days 01:59:51\n",
       "8    1 days 02:59:52\n",
       "Name: age, dtype: timedelta64[ns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.age.sort_values().head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# View:Like ratio that can score the video | view:comment ratio\n",
    "\n",
    "Have them all as a weighted ratio\n",
    "\n",
    "df['engagement'] = (df.likes + df.comment_count * 4 )/df.view_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creates engagement metric. not sure how to do weightd columns really\n",
    "#df['engagement'] = (df.view_count - df.likes) + (df.likes * 2) + (df.comment_count * 4) \n",
    "df['engagement'] = (df.likes + df.comment_count * 4 )/df.view_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      0.137897\n",
       "1      0.040702\n",
       "2      0.100914\n",
       "3      0.099933\n",
       "4      0.016842\n",
       "         ...   \n",
       "195    0.085803\n",
       "196    0.041752\n",
       "197    0.094759\n",
       "198    0.081997\n",
       "199    0.040904\n",
       "Name: engagement, Length: 200, dtype: float64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.engagement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Is the video sponsored?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6, 20)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#adds sponsored column based on appearance of word sponsored in the description\n",
    "df['sponsored'] = np.where(df.description.str.contains('sponsor'), 1, 0)\n",
    "df[df.description.str.contains('sponsored')].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Video language\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Percent of capital letters in title \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Age restricted\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we talk about subscribers we can take a look at age of channel\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to drive up subscribers is a slightly separate question but we can ask it\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At what point does the video view count pass the subscriber view count\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can target placing videos in/out of the top 25 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Letâ€™s think about tags and how many words they have in common with the descriptions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#countes number of tags given to video BEFORE stripping out extraneous things\n",
    "df['num_of_tags'] = df.tags.str.split('|').str.len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/x7/9w9mh0fd73zg4jr8l_9v9h840000gn/T/ipykernel_10477/1604276903.py:2: FutureWarning: The default value of regex will change from True to False in a future version. In addition, single character regular expressions will *not* be treated as literal strings when regex=True.\n",
      "  df.tags = df.tags.str.replace('|',\" \")\n"
     ]
    }
   ],
   "source": [
    "#gets rid of separator\n",
    "df.tags = df.tags.str.replace('|',\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports spacy\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creates the nlp object that is going to do the heavy lifting\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#uses the nlp object to convert the input text into a doc\n",
    "doc = nlp(df.loc[0].tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lil         PROPN     nmod      \n",
      "durk        PROPN     amod      \n",
      "lil         PROPN     nmod      \n",
      "durk        PROPN     compound  \n",
      "music       PROPN     nmod      \n",
      "lil         PROPN     nummod    \n",
      "durk        PROPN     compound  \n",
      "music       NOUN      compound  \n",
      "video       NOUN      nsubj     \n",
      "just        ADV       advmod    \n",
      "cause       VERB      mark      \n",
      "y           PRON      nsubj     \n",
      "all         PRON      appos     \n",
      "waited      VERB      ROOT      \n",
      "2           NUM       nummod    \n",
      "lil         NOUN      compound  \n",
      "durk        PROPN     dobj      \n",
      "2020        NUM       nummod    \n",
      "just        ADV       advmod    \n",
      "cause       VERB      mark      \n",
      "y'          PRON      nsubj     \n",
      "all         PRON      appos     \n",
      "waited      VERB      advcl     \n",
      "2           NUM       nummod    \n",
      "durkio      NOUN      dobj      \n",
      "smurkio     VERB      conj      \n",
      "lil         PROPN     compound  \n",
      "durk        PROPN     amod      \n",
      "official    ADJ       amod      \n",
      "drill       NOUN      dobj      \n",
      "drill       NOUN      compound  \n",
      "music       PROPN     compound  \n",
      "chicago     PROPN     compound  \n",
      "hip         NOUN      compound  \n",
      "hop         NOUN      dobj      \n",
      "only        ADV       advmod    \n",
      "the         DET       det       \n",
      "family      NOUN      compound  \n",
      "OTF         PROPN     appos     \n",
      "lil durk PERSON\n",
      "2 CARDINAL\n",
      "2 CARDINAL\n",
      "chicago GPE\n",
      "OTF ORG\n"
     ]
    }
   ],
   "source": [
    "#goes through tokens (words) in each doc\n",
    "for token in doc:\n",
    "    # Get the token text, part-of-speech tag and dependency label\n",
    "    token_text = token.text\n",
    "    token_pos = token.pos_\n",
    "    token_dep = token.dep_\n",
    "    # This is for formatting only\n",
    "    print(f\"{token_text:<12}{token_pos:<10}{token_dep:<10}\")\n",
    "\n",
    "# Iterate over the predicted entities\n",
    "for ent in doc.ents:\n",
    "    # Print the entity text and its label\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'music', 'drill', 'y', 'durkio', 'hip', '2', 'video', 'family', 'smurkio', 'the', 'OTF', 'only', 'waited', 'lil', 'cause', 'all', 'just', \"y'\", 'durk', 'official', 'hop', 'chicago', '2020'}\n"
     ]
    }
   ],
   "source": [
    "#creates set of unique words in doc \n",
    "tokens = set()\n",
    "for token in doc:\n",
    "    tokens.add(token.text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "docb = nlp(df.loc[4].description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The         DET       det       \n",
      "Inside      PROPN     compound  \n",
      "fellas      NOUN      nsubj     \n",
      "break       VERB      ROOT      \n",
      "down        ADP       prt       \n",
      "the         DET       det       \n",
      "Klay        PROPN     compound  \n",
      "-           PUNCT     punct     \n",
      "Dbook       NOUN      compound  \n",
      "interaction NOUN      dobj      \n",
      "and         CCONJ     cc        \n",
      "recap       VERB      conj      \n",
      "Warriors    PROPN     compound  \n",
      "-           PUNCT     punct     \n",
      "Suns        PROPN     dobj      \n",
      "on          ADP       prep      \n",
      "TNT         PROPN     pobj      \n",
      ".           PUNCT     punct     \n",
      "Watch       VERB      ROOT      \n",
      "highlights  NOUN      dobj      \n",
      "from        ADP       prep      \n",
      "Inside      ADP       prep      \n",
      "the         DET       det       \n",
      "NBA         PROPN     pobj      \n",
      "with        ADP       prep      \n",
      "Shaq        PROPN     pobj      \n",
      ",           PUNCT     punct     \n",
      "Charles     PROPN     compound  \n",
      "Barkley     PROPN     conj      \n",
      ",           PUNCT     punct     \n",
      "Kenny       PROPN     compound  \n",
      "Smith       PROPN     conj      \n",
      "and         CCONJ     cc        \n",
      "Ernie       PROPN     compound  \n",
      "Johnson     PROPN     conj      \n",
      "and         CCONJ     cc        \n",
      "more        ADJ       conj      \n",
      "!           PUNCT     punct     \n",
      "Subscribe   VERB      ccomp     \n",
      "now         ADV       advmod    \n",
      "to          PART      aux       \n",
      "be          AUX       auxpass   \n",
      "updated     VERB      relcl     \n",
      "on          ADP       prep      \n",
      "the         DET       det       \n",
      "latest      ADJ       amod      \n",
      "videos      NOUN      pobj      \n",
      ":           PUNCT     punct     \n",
      "https://www.youtube.com/nbaontnt?sub_confirmation=1X         nmod      \n",
      "          SPACE     dep       \n",
      "Connect     PROPN     nsubj     \n",
      "with        ADP       prep      \n",
      "NBA         PROPN     pobj      \n",
      "on          ADP       prep      \n",
      "TNT         PROPN     pobj      \n",
      ":           PUNCT     punct     \n",
      "           SPACE     dep       \n",
      "Follow      VERB      ROOT      \n",
      "NBA         PROPN     dobj      \n",
      "on          ADP       prep      \n",
      "TNT         PROPN     pobj      \n",
      "on          ADP       prep      \n",
      "Twitter     PROPN     pobj      \n",
      ":           PUNCT     punct     \n",
      "https://twitter.com/NBAonTNTNOUN      dobj      \n",
      "           SPACE     dep       \n",
      "Like        ADP       prep      \n",
      "NBA         PROPN     pobj      \n",
      "on          ADP       prep      \n",
      "TNT         PROPN     pobj      \n",
      "on          ADP       prep      \n",
      "Facebook    PROPN     pobj      \n",
      ":           PUNCT     punct     \n",
      "https://www.facebook.com/NBAONTNT/NOUN      dep       \n",
      "           SPACE     dep       \n",
      "Follow      VERB      appos     \n",
      "NBA         PROPN     dobj      \n",
      "on          ADP       prep      \n",
      "TNT         PROPN     pobj      \n",
      "on          ADP       prep      \n",
      "Instagram   PROPN     pobj      \n",
      ":           PUNCT     punct     \n",
      "https://www.instagram.com/nbaontnt/?hl=enX         dep       \n",
      "Warriors-Suns ORG\n",
      "TNT ORG\n",
      "NBA ORG\n",
      "Charles Barkley PERSON\n",
      "Kenny Smith PERSON\n",
      "Ernie Johnson PERSON\n",
      "NBA ORG\n",
      "TNT ORG\n",
      "TNT ORG\n",
      "NBA ORG\n",
      "TNT ORG\n",
      "TNT ORG\n",
      "Instagram ORG\n"
     ]
    }
   ],
   "source": [
    "for token in docb:\n",
    "    # Get the token text, part-of-speech tag and dependency label\n",
    "    token_text = token.text\n",
    "    token_pos = token.pos_\n",
    "    token_dep = token.dep_\n",
    "    # This is for formatting only\n",
    "    print(f\"{token_text:<12}{token_pos:<10}{token_dep:<10}\")\n",
    "\n",
    "# Iterate over the predicted entities\n",
    "for ent in docb.ents:\n",
    "    # Print the entity text and its label\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROPN\n",
      "NOUN\n",
      "VERB\n",
      "ADP\n",
      "PROPN\n",
      "NOUN\n",
      "NOUN\n",
      "CCONJ\n",
      "VERB\n",
      "PROPN\n",
      "PROPN\n",
      "ADP\n",
      "PROPN\n",
      "VERB\n",
      "NOUN\n",
      "ADP\n",
      "ADP\n",
      "PROPN\n",
      "ADP\n",
      "PROPN\n",
      "PROPN\n",
      "PROPN\n",
      "PROPN\n",
      "PROPN\n",
      "CCONJ\n",
      "PROPN\n",
      "PROPN\n",
      "CCONJ\n",
      "ADJ\n",
      "VERB\n",
      "ADV\n",
      "PART\n",
      "AUX\n",
      "VERB\n",
      "ADP\n",
      "ADJ\n",
      "NOUN\n",
      "X\n",
      "SPACE\n",
      "PROPN\n",
      "ADP\n",
      "PROPN\n",
      "ADP\n",
      "PROPN\n",
      "SPACE\n",
      "VERB\n",
      "PROPN\n",
      "ADP\n",
      "PROPN\n",
      "ADP\n",
      "PROPN\n",
      "NOUN\n",
      "SPACE\n",
      "ADP\n",
      "PROPN\n",
      "ADP\n",
      "PROPN\n",
      "ADP\n",
      "PROPN\n",
      "NOUN\n",
      "SPACE\n",
      "VERB\n",
      "PROPN\n",
      "ADP\n",
      "PROPN\n",
      "ADP\n",
      "PROPN\n",
      "X\n",
      "Warriors-Suns ORG\n",
      "TNT ORG\n",
      "NBA ORG\n",
      "Charles Barkley PERSON\n",
      "Kenny Smith PERSON\n",
      "Ernie Johnson PERSON\n",
      "NBA ORG\n",
      "TNT ORG\n",
      "TNT ORG\n",
      "NBA ORG\n",
      "TNT ORG\n",
      "TNT ORG\n",
      "Instagram ORG\n",
      "{'Follow', 'now', 'Like', 'and', 'NBA', 'Barkley', 'Dbook', 'Shaq', 'Facebook', 'on', 'Kenny', 'Suns', 'TNT', 'https://www.instagram.com/nbaontnt/?hl=en', '\\r', 'Subscribe', 'be', 'Connect', 'latest', 'highlights', 'to', 'Ernie', '\\r\\r', 'more', 'updated', 'Watch', 'videos', 'down', 'Klay', 'https://www.youtube.com/nbaontnt?sub_confirmation=1', 'https://twitter.com/NBAonTNT', 'break', 'recap', 'Johnson', 'interaction', 'Inside', 'from', 'with', 'Instagram', 'https://www.facebook.com/NBAONTNT/', 'fellas', 'Twitter', 'Charles', 'Warriors', 'Smith'}\n"
     ]
    }
   ],
   "source": [
    "tokens = set()\n",
    "for token in docb:\n",
    "    if token.pos_ not in ['SYM', 'PUNCT', 'DET']:\n",
    "        print(token.pos_)\n",
    "        tokens.add(token.text)\n",
    "\n",
    "for ent in docb.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warriors-Suns ORG\n",
      "TNT ORG\n",
      "NBA ORG\n",
      "Charles Barkley PERSON\n",
      "Kenny Smith PERSON\n",
      "Ernie Johnson PERSON\n",
      "NBA ORG\n",
      "TNT ORG\n",
      "TNT ORG\n",
      "NBA ORG\n",
      "TNT ORG\n",
      "TNT ORG\n",
      "Instagram ORG\n"
     ]
    }
   ],
   "source": [
    "for ent in docb.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    \"\"\" \n",
    "    Purpose:\n",
    "        to clean text input into function by removing duplicate words, punctuations, and other things\n",
    "    ---\n",
    "    Parameters:\n",
    "        text: a string\n",
    "    ---\n",
    "    Returns:\n",
    "        tokens: a set of words found in the input text\n",
    "    \"\"\"\n",
    "\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    doc = nlp(text)\n",
    "    tokens = set()\n",
    "    \n",
    "    for token in doc:\n",
    "        if token.pos_ not in ['SYM', 'PUNCT', 'DET']:\n",
    "            tokens.add(token.text)\n",
    "\n",
    "    for ent in doc.ents:\n",
    "        tokens.add(ent.text)\n",
    "\n",
    "    return tokens\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['cleaned_tags'] =  df['description'].apply(lambda x: clean_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['cleaned_desc'] = df['description'].apply(lambda x: clean_text(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Think about combining the countries top 25 lists and control for duplicates. \n",
    "* This way we can classify what videos have been a top 25 video \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create rank and top 25 categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['rank'] = df.index + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['top_25'] = np.where(df['rank'] < 26, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      1\n",
       "1      1\n",
       "2      1\n",
       "3      1\n",
       "4      1\n",
       "      ..\n",
       "195    0\n",
       "196    0\n",
       "197    0\n",
       "198    0\n",
       "199    0\n",
       "Name: top_25, Length: 200, dtype: int64"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.top_25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "38cca0c38332a56087b24af0bc80247f4fced29cb4f7f437d91dc159adec9c4e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
